Project: The "Scientist Agent" Memory ExperimentDate: 2025-11-14Authors: User & Gemini1. Executive Summary1.1. Overall ObjectiveTo scientifically measure and validate the impact of two distinct memory systems on an LLM agent's problem-solving efficiency:Focused Memory (Mid-Term): The value of a "clean" semantically-filtered context (RAG) vs. a "noisy" saturated context.Distilled Intuition (Long-Term): The value of fine-tuning an agent on "distilled summaries" of its experiences vs. the "raw logs" of those experiences.1.2. Core Research QuestionsE1: Does an agent with a semantically-focused RAG memory ("smart recall") solve problems more efficiently than an agent with a temporally-ordered, noisy memory ("full notebook")?E2: Can we train an agent's "intuition" (its base weights) to be a more effective user of its RAG memory? Is it more efficient to train this intuition on "distilled lessons" or "raw experience"?1.3. Local Technology Stack (M2 Pro)LLM Runner: OllamaAgent "Brain": Phi-3-mini-Instruct (via ollama Python library)Mid-Term Memory DB: sqlite-vecEmbedding Model: MiniLM (via sentence-transformers)Fine-Tuning: mlx (Apple's ML framework) with LoRA (Low-Rank Adaptation)2. Experiment 1: The Value of Focused Memory (RAG vs. Saturated)2.1. ObjectiveTo test if a "signal-focused" RAG context (intelligent retrieval) is superior to a "noise-filled" saturated context (temporal retrieval). This isolates the value of information relevance.2.2. HypothesisH₁ (Alternative): The RAG Agent (semantic retrieval) will require significantly fewer attempts to solve problems than the Saturated Agent.H₀ (Null): The RAG Agent will require an equal or greater number of attempts. (Attempts_RAG >= Attempts_Saturated)2.3. MethodologyPhase 1: Data Generation & Setup"Memory" Set (1000 Problems): Generate 1,000 unique (equation, data_points) pairs. (e.g., y=3x+2, y=x^2-5, etc.)."Test" Set (100 Problems): Generate 100 new, unseen (equation, data_points) pairs.CRITICAL (Data Separation): There must be zero overlap between the Memory and Test sets to prevent data leakage. The Test Set must be a true test of generalization.Populate RAG DB: Run a basic agent on the 1,000 "Memory Set" problems. For each solved problem, store a "solution memo" (e.g., "Problem: y=3x+2, Final Guess: 3*x + 2, Attempts: 4, Failures: [2*x+1, x+5, ...]") in the sqlite-vec database. This 1,000-memo database is the "Mid-Term Memory" library.Phase 2: The "Gauntlet" (Evaluation)Run both agents below on the 100-problem "Test" Set.Both agents have access to the same 1,000-memo RAG database from Phase 1.Set a max_attempts limit (e.g., 20) for each problem. A failure is recorded as 20.Phase 3: Experimental GroupsControl ("Saturated Agent"):Retrieval: For each new attempt, its context is "stuffed" with as many of the most recent memos from the DB as can fit into a fixed token budget (e.g., 2000 tokens).Rationale: This simulates a "noisy" context full of recent but irrelevant information. It must find the "needle" (if any) in the "haystack."Experimental ("RAG Agent"):Retrieval: For each new attempt, its "current thought" or the "problem data" is embedded. It retrieves the most semantically similar memos that fit into the same 2000-token budget.Rationale: This simulates a "clean" context with only (theoretically) relevant information.2.4. Metrics & AnalysisPrimary Metric: attempts_to_solve (a list of 100 numbers for each agent).Statistical Test: A paired t-test (scipy.stats.ttest_rel) comparing attempts_control vs. attempts_experimental, using alternative='less'.Success Condition: A p_value < 0.05 indicates a statistically significant result, allowing us to reject the null hypothesis and conclude that "focused memory" (RAG) is superior to "saturated memory."3. Experiment 2: The Value of Distilled Intuition (LoRA)3.1. ObjectiveTo test if an agent's "intuition" (base weights) can be fine-tuned to be a more effective "operator" of the full RAG system, and whether "distilled lessons" or "raw logs" are the superior training data for this.3.2. HypothesisWe are testing a cost/performance trade-off. We will have a "Success Condition" if either of the following is true (and the other is not significantly worse).H₁ (Efficiency Win): Training_Cost(S) < Training_Cost(R) AND Performance(S) >= Performance(R)H₂ (Performance Win): Performance(S) < Performance(R) AND Training_Cost(S) <= Training_Cost(R)(Where S=Summary-Trained, R=Raw-Log-Trained, Cost is time/compute, and Performance is attempts_to_solve)3.3. MethodologyPhase 1: Data Generation (Using 1000-Problem "Memory Set")Dataset S (Summaries): For each of the 1,000 solved problems, use the Phi-3 agent to write a one-sentence "distilled lesson" or "strategic hunch."Example: Problem: y=x^2+2. Lesson: When y-values grow non-linearly, test polynomial functions like 'x**2' first.Dataset R (Raw Logs): Save the full, messy transcripts for all 1,000 solved problems, including every prompt, every failed guess, every loss score, and every RAG retrieval.Phase 2: Fine-Tuning (using mlx + LoRA)Start with the same base Phi-3-mini model for all three agents.Model-S (Summary): Fine-tune (LoRA) the base model on Dataset S.Model-R (Raw Log): Fine-tune (LoRA) the base model on Dataset R.Model-C (Control): This is the original, untuned base Phi-3-mini.Phase 3: The "Gauntlet" (Evaluation)CRITICAL (Fair Test): We are not testing in "amnesiac" mode. This test evaluates which model is the best operator of the full RAG system.Run all three agents (Model-S, Model-R, Model-C) on the 100-problem "Test" Set.All three agents have full access to the 1,000-memo RAG database created in E1.3.4. Metrics & AnalysisHunch Prompting: The agent's prompt must be structured to use its hunch (e.g., "First, state your initial hunch. Then, use that hunch to retrieve memories. Then, make your guess."). This ensures we are testing the model's ability to form a better RAG query.Efficiency Metrics:dataset_file_size_S vs. dataset_file_size_Rtraining_wall_clock_time_S vs. training_wall_clock_time_R (on M2)Performance Metrics:attempts_to_solve_S, attempts_to_solve_R, attempts_to_solve_CRun paired t-tests for all three pairs (S vs. R, S vs. C, R vs. C).Success Condition:We declare an "Efficiency Win" if Training_Cost(S) is significantly lower than Training_Cost(R) while Performance(S) is not significantly worse (i.e., p_value > 0.05 on a "less" test).We declare a "Performance Win" if Performance(S) is significantly better than Performance(R) while Training_Cost(S) is not significantly worse.